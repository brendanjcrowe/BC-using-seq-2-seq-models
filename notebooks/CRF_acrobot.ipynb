{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Field implementation\n",
    "## Using small number of demonstrations, no adversarial data\n",
    "### Now using a discritivation of the state space so that I can represent interesting state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Expert Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-93.24"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experts import Expert\n",
    "\n",
    "ENV_NAME = 'Acrobot-v1'\n",
    "expert = Expert(ENV_NAME)\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "data, avg_reward, splits = expert.generate_data(num_episodes=100)\n",
    "sequences = np.split(data, splits)[0:-1]\n",
    "avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9999942148146757 0.9999999677597143\n",
      "-0.9999999905199425 0.9999997422606326\n",
      "-0.9999996651775235 0.9999998237451254\n",
      "-0.9999999960117508 0.9999999832593169\n",
      "-9.689276561534452 10.892076040873079\n",
      "-21.21953680868765 20.16736924230797\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    \n",
    "    print(np.min(data[:,i]), np.max(data[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99828285,  0.05857777,  0.99552911, -0.09445523,  0.05452896,\n",
       "        0.03578921])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature function, and functions to transform data into correct for for crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats(seq, k):\n",
    "    \n",
    "    return {\n",
    "            'f11': np.format_float_positional(seq[k-1][0], 2, unique=False),\n",
    "            'f12': np.format_float_positional(seq[k][0], 2, unique=False),\n",
    "            'f21': np.format_float_positional(seq[k-1][1], 2, unique=False),\n",
    "            'f22': np.format_float_positional(seq[k][1], 2, unique=False),\n",
    "            'f31': np.format_float_positional(seq[k-1][2], 2, unique=False),\n",
    "            'f32': np.format_float_positional(seq[k][2], 2, unique=False),\n",
    "            'f41': np.format_float_positional(seq[k-1][3], 2, unique=False),\n",
    "            'f42': np.format_float_positional(seq[k][3], 2, unique=False),\n",
    "            'f51': np.format_float_positional(seq[k-1][4], 1, unique=False),\n",
    "            'f52': np.format_float_positional(seq[k][4], 1, unique=False),\n",
    "            'f61': np.format_float_positional(seq[k-1][5], 1, unique=False),\n",
    "            'f62': np.format_float_positional(seq[k][5], 1, unique=False),\n",
    "            'first': k == 0\n",
    "            #'last': k == len(seq)-1\n",
    "            #'bias': True\n",
    "    }\n",
    "\n",
    "def labs(seq, k):\n",
    "    \n",
    "    return str(seq[k][6])\n",
    "    \n",
    "def seq_to_feats(seq):\n",
    "    \n",
    "    return [feats(seq, k) for k in range(len(seq))]\n",
    "\n",
    "def seq_to_labs(seq):\n",
    "    \n",
    "    return [labs(seq, k) for k in range(len(seq))]\n",
    "\n",
    "X_train = [seq_to_feats(seq) for seq in sequences]\n",
    "y_train = [seq_to_labs(seq) for seq in sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 100/100 [00:00<00:00, 1577.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 1\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 5366\n",
      "Seconds required: 0.047\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 0.000000\n",
      "num_memories: 6\n",
      "max_iterations: 25000\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.01  loss=3938.41  active=5164  feature_norm=1.00\n",
      "Iter 2   time=0.01  loss=3129.21  active=5366  feature_norm=1.55\n",
      "Iter 3   time=0.01  loss=2631.56  active=5366  feature_norm=2.14\n",
      "Iter 4   time=0.00  loss=2456.93  active=5366  feature_norm=2.65\n",
      "Iter 5   time=0.00  loss=2294.78  active=5366  feature_norm=3.11\n",
      "Iter 6   time=0.00  loss=2004.49  active=5366  feature_norm=4.02\n",
      "Iter 7   time=0.00  loss=1477.73  active=5366  feature_norm=5.79\n",
      "Iter 8   time=0.00  loss=868.85   active=5366  feature_norm=8.18\n",
      "Iter 9   time=0.00  loss=721.55   active=5366  feature_norm=9.44\n",
      "Iter 10  time=0.00  loss=491.90   active=5366  feature_norm=12.38\n",
      "Iter 11  time=0.00  loss=344.97   active=5366  feature_norm=16.33\n",
      "Iter 12  time=0.00  loss=258.39   active=5366  feature_norm=19.16\n",
      "Iter 13  time=0.00  loss=222.85   active=5366  feature_norm=19.34\n",
      "Iter 14  time=0.00  loss=211.25   active=5366  feature_norm=20.22\n",
      "Iter 15  time=0.00  loss=179.20   active=5366  feature_norm=23.66\n",
      "Iter 16  time=0.00  loss=129.44   active=5366  feature_norm=28.41\n",
      "Iter 17  time=0.00  loss=94.45    active=5366  feature_norm=33.80\n",
      "Iter 18  time=0.00  loss=56.99    active=5366  feature_norm=41.11\n",
      "Iter 19  time=0.00  loss=45.99    active=5366  feature_norm=51.61\n",
      "Iter 20  time=0.00  loss=25.14    active=5366  feature_norm=56.38\n",
      "Iter 21  time=0.00  loss=22.01    active=5366  feature_norm=57.44\n",
      "Iter 22  time=0.00  loss=16.07    active=5366  feature_norm=62.23\n",
      "Iter 23  time=0.00  loss=10.04    active=5366  feature_norm=68.70\n",
      "Iter 24  time=0.00  loss=3.21     active=5366  feature_norm=79.04\n",
      "Iter 25  time=0.00  loss=1.84     active=5366  feature_norm=89.58\n",
      "Iter 26  time=0.01  loss=1.12     active=5366  feature_norm=94.22\n",
      "Iter 27  time=0.00  loss=0.70     active=5366  feature_norm=99.92\n",
      "Iter 28  time=0.00  loss=0.44     active=5366  feature_norm=108.48\n",
      "Iter 29  time=0.01  loss=0.18     active=5366  feature_norm=115.97\n",
      "Iter 30  time=0.00  loss=0.13     active=5366  feature_norm=120.49\n",
      "Iter 31  time=0.00  loss=0.06     active=5366  feature_norm=130.50\n",
      "Iter 32  time=0.00  loss=0.03     active=5366  feature_norm=139.44\n",
      "Iter 33  time=0.00  loss=0.01     active=5366  feature_norm=149.52\n",
      "Iter 34  time=0.00  loss=0.01     active=5366  feature_norm=159.94\n",
      "Iter 35  time=0.00  loss=0.00     active=5366  feature_norm=167.57\n",
      "Iter 36  time=0.01  loss=0.00     active=5366  feature_norm=173.38\n",
      "Iter 37  time=0.00  loss=0.00     active=5366  feature_norm=182.50\n",
      "L-BFGS resulted in convergence\n",
      "Total seconds required for training: 0.155\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 5366 (5366)\n",
      "Number of active attributes: 2681 (2681)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.002\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=True, all_possible_transitions=True,\n",
       "    c2=0, keep_tempfiles=None, max_iterations=25000, verbose=1)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=25000,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True,\n",
    "    verbose=1,\n",
    "    c2=0\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(np.hstack([np.array(seq_hat, dtype=np.float64) for seq_hat in crf.predict(X_train)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine how well the learned policy performs in the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for j in range(100):\n",
    "    sequence = np.empty((400, 6))\n",
    "    sequence[0] = env.reset()\n",
    "    i = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        \n",
    "        i += 1\n",
    "        features = seq_to_feats(sequence[0:i])\n",
    "        #print(features)\n",
    "        policy = int(float(crf.predict_single(features)[-1]))\n",
    "        observation, reward, done, info = env.step(policy)\n",
    "        sequence[i] = observation\n",
    "        score += reward\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "       \n",
    "        \n",
    "    \n",
    "    rewards.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-86.35, 20.33685078865457)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for j in range(1):\n",
    "    sequence = np.empty((201, 6))\n",
    "    sequence[0] = env.reset()\n",
    "    i = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        \n",
    "        i += 1\n",
    "        features = seq_to_feats(sequence[0:i])\n",
    "        #print(features)\n",
    "        policy = int(float(crf.predict_single(features)[-1]))\n",
    "        observation, reward, done, info = env.step(policy)\n",
    "        sequence[i] = observation\n",
    "        score += reward\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "       \n",
    "        \n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(crf, open('crf_models/acrobot_no_adversarial.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with basic logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='none', multi_class='multinomial', fit_intercept=True).fit(data[:,0:2], data[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.empty((100,))\n",
    "for j in range(100):\n",
    "    sequence = np.empty((200, 2))\n",
    "    observation = env.reset().reshape(1, -1)\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "\n",
    "        #features = seq_to_feats(sequence[0:i])\n",
    "        policy = model.predict(observation).astype(int)[0]\n",
    "        observation, reward, done, info = env.step(policy)\n",
    "        observation = observation.reshape(1, -1)\n",
    "        #sequence[i] = observation\n",
    "        score += reward\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "        #i += 1\n",
    "        \n",
    "    \n",
    "    scores[j] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score -110.86\n",
      "The less negative number is better\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Logistic Regression score', np.mean(scores))\n",
    "print('The less negative number is better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "sns.scatterplot(data.pos, data.vel, hue=model.predict(data.drop(['action', 'reward'], axis=1)), palette=['red', 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(crf, open('crf_mountain_car_no_adversarial', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Field implementation\n",
    "## Using small number of demonstrations, no adversarial data\n",
    "### Now using a discritivation of the state space so that I can represent interesting state features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/brendanjcrowe/anaconda3/envs/seq/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gym\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Expert Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from experts import Expert\n",
    "\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "expert = Expert(ENV_NAME)\n",
    "env = gym.make(ENV_NAME)\n",
    "\n",
    "data, avg_reward, splits = expert.generate_data(num_episodes=100)\n",
    "sequences = np.split(data, splits)[0:-1]\n",
    "avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8698980070012878 0.9011965660492729\n",
      "-0.4299339916568309 0.4228662862299207\n",
      "-0.04953431719218638 0.04912509795413926\n",
      "-0.3782617947520103 0.39411458878306904\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    \n",
    "    print(np.min(data[:,i]), np.max(data[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature function, and functions to transform data into correct for for crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats(seq, k):\n",
    "    \n",
    "    return {\n",
    "            'f11': np.format_float_positional(seq[k-1][0], 1, unique=False),\n",
    "            'f12': np.format_float_positional(seq[k][0], 1, unique=False),\n",
    "            'f21': np.format_float_positional(seq[k-1][1], 2, unique=False),\n",
    "            'f22': np.format_float_positional(seq[k][1], 2, unique=False),\n",
    "            'f31': np.format_float_positional(seq[k-1][2], 2, unique=False),\n",
    "            'f32': np.format_float_positional(seq[k][2], 2, unique=False),\n",
    "            'f41': np.format_float_positional(seq[k-1][3], 2, unique=False),\n",
    "            'f42': np.format_float_positional(seq[k][3], 2, unique=False),\n",
    "            'first': k == 0,\n",
    "            #'last': k == len(seq)-1\n",
    "            #'bias': True\n",
    "    }\n",
    "\n",
    "def labs(seq, k):\n",
    "    \n",
    "    return str(seq[k][4])\n",
    "    \n",
    "def seq_to_feats(seq):\n",
    "    \n",
    "    return [feats(seq, k) for k in range(len(seq))]\n",
    "\n",
    "def seq_to_labs(seq):\n",
    "    \n",
    "    return [labs(seq, k) for k in range(len(seq))]\n",
    "\n",
    "X_train = [seq_to_feats(seq) for seq in sequences]\n",
    "y_train = [seq_to_labs(seq) for seq in sequences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 100/100 [00:00<00:00, 1108.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 1\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 672\n",
      "Seconds required: 0.031\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 0.000000\n",
      "num_memories: 6\n",
      "max_iterations: 25000\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.01  loss=9442.20  active=652   feature_norm=1.00\n",
      "Iter 2   time=0.01  loss=5921.02  active=672   feature_norm=2.38\n",
      "Iter 3   time=0.01  loss=3413.21  active=672   feature_norm=4.39\n",
      "Iter 4   time=0.00  loss=2482.27  active=672   feature_norm=6.34\n",
      "Iter 5   time=0.00  loss=1994.08  active=672   feature_norm=8.61\n",
      "Iter 6   time=0.00  loss=1735.19  active=672   feature_norm=9.32\n",
      "Iter 7   time=0.00  loss=1563.41  active=672   feature_norm=10.51\n",
      "Iter 8   time=0.00  loss=1380.17  active=672   feature_norm=12.45\n",
      "Iter 9   time=0.00  loss=1151.33  active=672   feature_norm=15.60\n",
      "Iter 10  time=0.01  loss=1015.64  active=672   feature_norm=18.02\n",
      "Iter 11  time=0.00  loss=826.28   active=672   feature_norm=22.15\n",
      "Iter 12  time=0.01  loss=714.89   active=672   feature_norm=26.09\n",
      "Iter 13  time=0.01  loss=571.28   active=672   feature_norm=31.65\n",
      "Iter 14  time=0.01  loss=410.68   active=672   feature_norm=38.48\n",
      "Iter 15  time=0.01  loss=294.72   active=672   feature_norm=52.23\n",
      "Iter 16  time=0.01  loss=263.79   active=672   feature_norm=55.11\n",
      "Iter 17  time=0.01  loss=211.84   active=672   feature_norm=62.20\n",
      "Iter 18  time=0.00  loss=163.32   active=672   feature_norm=69.70\n",
      "Iter 19  time=0.01  loss=142.63   active=672   feature_norm=76.06\n",
      "Iter 20  time=0.01  loss=108.52   active=672   feature_norm=84.36\n",
      "Iter 21  time=0.00  loss=88.99    active=672   feature_norm=91.55\n",
      "Iter 22  time=0.00  loss=66.02    active=672   feature_norm=101.58\n",
      "Iter 23  time=0.00  loss=42.69    active=672   feature_norm=116.22\n",
      "Iter 24  time=0.00  loss=30.74    active=672   feature_norm=133.61\n",
      "Iter 25  time=0.00  loss=21.68    active=672   feature_norm=136.87\n",
      "Iter 26  time=0.00  loss=17.88    active=672   feature_norm=141.44\n",
      "Iter 27  time=0.00  loss=12.45    active=672   feature_norm=154.32\n",
      "Iter 28  time=0.00  loss=4.53     active=672   feature_norm=178.69\n",
      "Iter 29  time=0.00  loss=1.60     active=672   feature_norm=209.71\n",
      "Iter 30  time=0.00  loss=0.85     active=672   feature_norm=215.15\n",
      "Iter 31  time=0.00  loss=0.66     active=672   feature_norm=218.88\n",
      "Iter 32  time=0.00  loss=0.36     active=672   feature_norm=230.61\n",
      "Iter 33  time=0.00  loss=0.21     active=672   feature_norm=246.91\n",
      "Iter 34  time=0.00  loss=0.13     active=672   feature_norm=255.21\n",
      "Iter 35  time=0.00  loss=0.07     active=672   feature_norm=266.74\n",
      "Iter 36  time=0.00  loss=0.04     active=672   feature_norm=279.59\n",
      "Iter 37  time=0.00  loss=0.02     active=672   feature_norm=293.28\n",
      "Iter 38  time=0.00  loss=0.01     active=672   feature_norm=310.57\n",
      "Iter 39  time=0.00  loss=0.01     active=672   feature_norm=337.86\n",
      "Iter 40  time=0.00  loss=0.00     active=672   feature_norm=350.44\n",
      "L-BFGS resulted in convergence\n",
      "Total seconds required for training: 0.195\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 672 (672)\n",
      "Number of active attributes: 334 (334)\n",
      "Number of active labels: 2 (2)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.001\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=True, all_possible_transitions=True,\n",
       "    c2=0, keep_tempfiles=None, max_iterations=25000, verbose=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=25000,\n",
    "    all_possible_transitions=True,\n",
    "    all_possible_states=True,\n",
    "    verbose=1,\n",
    "    c2=0\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = np.array(np.hstack([np.array(seq_hat, dtype=np.float64) for seq_hat in crf.predict(X_train)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine how well the learned policy performs in the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for j in range(100):\n",
    "    sequence = np.empty((201, 4))\n",
    "    sequence[0] = env.reset()\n",
    "    i = 0\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        \n",
    "        i += 1\n",
    "        features = seq_to_feats(sequence[0:i])\n",
    "        #print(features)\n",
    "        policy = int(float(crf.predict_single(features)[-1]))\n",
    "        observation, reward, done, info = env.step(policy)\n",
    "        sequence[i] = observation\n",
    "        score += reward\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "       \n",
    "        \n",
    "    \n",
    "    rewards.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163.61, 56.48077460516986)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards), np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(crf, open('crf_models/cart_pole_no_adversarial.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with basic logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(penalty='none', multi_class='multinomial', fit_intercept=True).fit(data[:,0:2], data[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.empty((100,))\n",
    "for j in range(100):\n",
    "    sequence = np.empty((200, 2))\n",
    "    observation = env.reset().reshape(1, -1)\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "\n",
    "        #features = seq_to_feats(sequence[0:i])\n",
    "        policy = model.predict(observation).astype(int)[0]\n",
    "        observation, reward, done, info = env.step(policy)\n",
    "        observation = observation.reshape(1, -1)\n",
    "        #sequence[i] = observation\n",
    "        score += reward\n",
    "#         env.render()\n",
    "#         time.sleep(0.01)\n",
    "        #i += 1\n",
    "        \n",
    "    \n",
    "    scores[j] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression score -110.86\n",
      "The less negative number is better\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Logistic Regression score', np.mean(scores))\n",
    "print('The less negative number is better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "sns.scatterplot(data.pos, data.vel, hue=model.predict(data.drop(['action', 'reward'], axis=1)), palette=['red', 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(crf, open('crf_mountain_car_no_adversarial', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
